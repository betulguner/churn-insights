# Google BigQuery Kurulum Rehberi
## AI-Driven Customer Insights Platform

### ğŸ¯ AmaÃ§
Bu rehber, PostgreSQL veritabanÄ±ndaki mÃ¼ÅŸteri churn verilerini Google BigQuery'e senkronize etmek iÃ§in gerekli adÄ±mlarÄ± iÃ§erir.

### ğŸ“‹ Ã–n Gereksinimler
- Google Cloud hesabÄ±
- Google Cloud Project
- BigQuery API aktif
- Service Account ve credentials

## ğŸš€ AdÄ±m 1: Google Cloud Project OluÅŸturma

### 1.1 Google Cloud Console'a GiriÅŸ
1. [Google Cloud Console](https://console.cloud.google.com/) adresine gidin
2. Google hesabÄ±nÄ±zla giriÅŸ yapÄ±n
3. Yeni proje oluÅŸturun veya mevcut projeyi seÃ§in

### 1.2 Proje OluÅŸturma
```bash
# Proje adÄ± Ã¶nerisi
Project Name: churn-analysis-platform
Project ID: churn-analysis-platform-2025
```

## ğŸ”§ AdÄ±m 2: BigQuery API'yi AktifleÅŸtirme

### 2.1 API Library'den AktifleÅŸtirme
1. Google Cloud Console'da **APIs & Services > Library** bÃ¶lÃ¼mÃ¼ne gidin
2. "BigQuery API" arayÄ±n
3. **ENABLE** butonuna tÄ±klayÄ±n

### 2.2 Gerekli API'ler
- BigQuery API
- BigQuery Storage API
- Cloud Storage API (opsiyonel)

## ğŸ” AdÄ±m 3: Service Account OluÅŸturma

### 3.1 Service Account OluÅŸturma
1. **IAM & Admin > Service Accounts** bÃ¶lÃ¼mÃ¼ne gidin
2. **CREATE SERVICE ACCOUNT** butonuna tÄ±klayÄ±n
3. DetaylarÄ± doldurun:
   ```
   Service account name: churn-etl-service
   Service account ID: churn-etl-service
   Description: Service account for ETL operations
   ```

### 3.2 Rolleri Atama
Service account'a ÅŸu rolleri atayÄ±n:
- **BigQuery Admin**
- **BigQuery Data Editor**
- **BigQuery Job User**

### 3.3 Credentials Ä°ndirme
1. Service account'a tÄ±klayÄ±n
2. **KEYS** sekmesine gidin
3. **ADD KEY > Create new key** seÃ§in
4. **JSON** formatÄ±nÄ± seÃ§in
5. DosyayÄ± indirin ve gÃ¼venli bir yere kaydedin

## ğŸ“ AdÄ±m 4: Credentials YapÄ±landÄ±rmasÄ±

### 4.1 Environment Variable Ayarlama
```bash
# macOS/Linux
export GOOGLE_APPLICATION_CREDENTIALS="/Users/betulguner/Documents/Churn/churn-471614-a2cdb1c78845.json"

# Windows
set GOOGLE_APPLICATION_CREDENTIALS=C:\path\to\your\credentials.json
```

### 4.2 Python'da Credentials Kullanma
```python
from google.cloud import bigquery
from google.oauth2 import service_account

# Credentials dosyasÄ± ile client oluÅŸturma
credentials = service_account.Credentials.from_service_account_file(
    'path/to/credentials.json'
)
client = bigquery.Client(credentials=credentials, project='your-project-id')
```

## ğŸ—„ï¸ AdÄ±m 5: BigQuery Dataset OluÅŸturma

### 5.1 BigQuery Console'da Dataset OluÅŸturma
1. [BigQuery Console](https://console.cloud.google.com/bigquery) adresine gidin
2. Projenizi seÃ§in
3. **CREATE DATASET** butonuna tÄ±klayÄ±n
4. Dataset detaylarÄ±nÄ± doldurun:
   ```
   Dataset ID: churn_analysis
   Data location: US (multi-region)
   Description: Customer churn analysis dataset
   ```

### 5.2 Python ile Dataset OluÅŸturma
```python
from google.cloud import bigquery

client = bigquery.Client(project='your-project-id')
dataset_id = 'churn_analysis'

dataset = bigquery.Dataset(f"{client.project}.{dataset_id}")
dataset.location = "US"
dataset.description = "AI-Driven Customer Insights Platform Dataset"

dataset = client.create_dataset(dataset, exists_ok=True)
print(f"Dataset {dataset_id} created.")
```

## ğŸ“Š AdÄ±m 6: TablolarÄ± OluÅŸturma

### 6.1 Schema DosyasÄ±nÄ± Kullanma
```bash
# BigQuery schema dosyasÄ±nÄ± dÃ¼zenle
# bigquery_schema.sql dosyasÄ±ndaki 'your-project-id' kÄ±smÄ±nÄ± gerÃ§ek project ID ile deÄŸiÅŸtir

# Python scripti ile tablolarÄ± oluÅŸtur
python bigquery_integration.py
```

### 6.2 Manuel Tablo OluÅŸturma
BigQuery Console'da SQL Editor'Ã¼ kullanarak:
```sql
-- Ã–rnek tablo oluÅŸturma
CREATE TABLE `your-project-id.churn_analysis.customer_demographics` (
    customer_id STRING NOT NULL,
    gender STRING NOT NULL,
    senior_citizen BOOL NOT NULL,
    partner BOOL NOT NULL,
    dependents BOOL NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
)
PARTITION BY DATE(created_at)
CLUSTER BY customer_id, gender;
```

## ğŸ”„ AdÄ±m 7: Veri Senkronizasyonu

### 7.1 Python Scripti ile Senkronizasyon
```python
# bigquery_integration.py dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±r
python bigquery_integration.py
```

### 7.2 Manuel Veri YÃ¼kleme
1. BigQuery Console'da tabloya tÄ±klayÄ±n
2. **UPLOAD** butonuna tÄ±klayÄ±n
3. CSV dosyasÄ±nÄ± seÃ§in
4. Schema'yÄ± doÄŸrulayÄ±n
5. **CREATE TABLE** butonuna tÄ±klayÄ±n

## ğŸ“ˆ AdÄ±m 8: Analiz SorgularÄ±

### 8.1 Temel Churn Analizi
```sql
SELECT 
    contract_type,
    internet_service,
    COUNT(*) as total_customers,
    COUNT(CASE WHEN churn_status = TRUE THEN 1 END) as churned_customers,
    ROUND(COUNT(CASE WHEN churn_status = TRUE THEN 1 END) * 100.0 / COUNT(*), 2) as churn_rate
FROM `your-project-id.churn_analysis.customer_complete_view`
GROUP BY contract_type, internet_service
ORDER BY churn_rate DESC;
```

### 8.2 MÃ¼ÅŸteri Segmentasyonu
```sql
SELECT 
    segment_name,
    COUNT(*) as customer_count,
    AVG(cltv_score) as avg_cltv,
    AVG(risk_score) as avg_risk,
    COUNT(CASE WHEN churn_status = TRUE THEN 1 END) as churned_count
FROM `your-project-id.churn_analysis.customer_complete_view`
GROUP BY segment_name
ORDER BY avg_cltv DESC;
```

## ğŸ” AdÄ±m 9: Veri Kalitesi Kontrolleri

### 9.1 Completeness Check
```sql
SELECT 
    'customer_demographics' as table_name,
    'completeness' as metric_name,
    (COUNT(*) - COUNTIF(customer_id IS NULL OR gender IS NULL)) / COUNT(*) as metric_value
FROM `your-project-id.churn_analysis.customer_demographics`;
```

### 9.2 Uniqueness Check
```sql
SELECT 
    'customer_demographics' as table_name,
    'uniqueness' as metric_name,
    COUNT(DISTINCT customer_id) / COUNT(*) as metric_value
FROM `your-project-id.churn_analysis.customer_demographics`;
```

## ğŸ’° AdÄ±m 10: Maliyet Optimizasyonu

### 10.1 Partitioning
- TÃ¼m tablolar `created_at` tarihine gÃ¶re partition edildi
- Bu, sorgu performansÄ±nÄ± artÄ±rÄ±r ve maliyeti dÃ¼ÅŸÃ¼rÃ¼r

### 10.2 Clustering
- Tablolar Ã¶nemli sÃ¼tunlara gÃ¶re cluster edildi
- Bu, benzer verilerin fiziksel olarak yakÄ±n olmasÄ±nÄ± saÄŸlar

### 10.3 Free Tier Limitleri
- **AylÄ±k 1TB sorgu verisi** Ã¼cretsiz
- **10GB depolama** Ã¼cretsiz
- **1TB aylÄ±k aktarÄ±m** Ã¼cretsiz

## ğŸš¨ Sorun Giderme

### Sorun 1: Authentication HatasÄ±
```bash
# Credentials dosyasÄ±nÄ±n doÄŸru yolda olduÄŸunu kontrol edin
echo $GOOGLE_APPLICATION_CREDENTIALS

# Credentials dosyasÄ±nÄ±n geÃ§erli olduÄŸunu kontrol edin
gcloud auth application-default print-access-token
```

### Sorun 2: Permission HatasÄ±
- Service account'a gerekli rolleri atadÄ±ÄŸÄ±nÄ±zdan emin olun
- BigQuery Admin rolÃ¼ gerekli

### Sorun 3: Dataset BulunamadÄ±
- Dataset'in doÄŸru projede oluÅŸturulduÄŸunu kontrol edin
- Project ID'nin doÄŸru olduÄŸunu kontrol edin

## ğŸ“ Sonraki AdÄ±mlar

### 2. Hafta HazÄ±rlÄ±ÄŸÄ±
- BigQuery'deki veriler ML modelleri iÃ§in hazÄ±r
- Scikit-learn ile churn prediction modelleri
- BigQuery ML ile doÄŸrudan ML modelleri

### 3. Hafta HazÄ±rlÄ±ÄŸÄ±
- RAG chatbot iÃ§in BigQuery entegrasyonu
- LangChain ile natural language query processing
- Real-time analytics dashboard

## ğŸ“ Destek

Sorun yaÅŸarsanÄ±z:
1. Google Cloud Console'da hata loglarÄ±nÄ± kontrol edin
2. BigQuery job history'yi inceleyin
3. Service account permissions'larÄ± kontrol edin

---

**Not**: Bu rehber, BigQuery free tier limitleri iÃ§inde kalacak ÅŸekilde tasarlanmÄ±ÅŸtÄ±r. Production ortamÄ±nda ek optimizasyonlar gerekebilir.
